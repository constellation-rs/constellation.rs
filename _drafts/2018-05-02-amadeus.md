---
layout: post
title:  "Amadeus: Harmonious distributed data processing in Rust"
date:   2019-11-07 11:34:21 +0100
categories: [Amadeus]
highlight_url: /blog
---

**Draft**

I’ve spent the past few weeks integrating and testing a range of data sources into Amadeus, my experimental library for running data processing workloads across a cluster. I’m quite excited as it’s starting to look useful for workloads that I’ve previously encountered, so I thought I’d write up what I’ve got so far.

**Amadeus aims to be a batteries-included, lightning-fast library for data processing and analytics, with a focus on streaming computation and strong reliability guarantees.** It aims to make writing data processing and analytics jobs as easy as possible by abstracting over common data sources, and leveraging the [iterator pattern](https://doc.rust-lang.org/book/ch13-02-iterators.html) for describing computation. This computation is then run over multiple threads or, experimentally, over processes across a cluster.

<!--
to declare arbitrarily complex transformations. , which are then made to run distributed.

 enable programs that leverage the [iterator pattern](https://doc.rust-lang.org/book/ch13-02-iterators.html) to declare potentially complex transformations, which are then made to run distributed

enable sophisticated analytics, over common data formats, leveraging threads for parallelism 



 It aims to Just Work: on a single machine using threads for parallelism; and across a cluster using processes for parallelism.
 -->

To that end, I’ve been working on three main components. The rest of this series of blog posts will explore what these look like and mean in practise, but here’s a quick summary:

 * **A process pool**. Dynamically allocating pieces of work to a pool of threads is what powers Rayon’s [“potential parallelism”](http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/). Whereas a pool of threads is limited to one machine[^shared-memory], **a pool of processes can be distributed over multiple machines in a cluster**. <!-- a process pool enables work to be distributed ... -->
 * **Abstraction over different data sources and sinks**. Data processing typically involves ingesting data from a range of different data sources, from structured or unstructured files, to databases, to APIs. A standard interface, and a [Canonical Data Model](https://en.wikipedia.org/wiki/Canonical_model) that supersets their native data types, can **make switching between Parquet, CSV, JSON, PostgreSQL etc seamless**. <!--An interface standardised in a trait,  These connectors should abstracted over by a  Standardised but powerful interface to data sources/sinks, as well as a [Canonical Data Model](https://en.wikipedia.org/wiki/Canonical_model) to bridge them. **Switching your reading/writing between Parquet, CSV, JSON, PostgreSQL etc should be seamless.**-->
 * **Streaming analytics functions**. Most data processing that I’ve seen done successfully at large scale fits within the [streaming computation model](https://en.wikipedia.org/wiki/Streaming_algorithm). While not as powerful as the full [DataFrame](https://pandas.pydata.org/pandas-docs/stable/getting_started/dsintro.html#dataframe) model, it does provide **highly predictable memory consumption[^streaming-memory-consumption] enabling strong reliability guarantees**. As such it lays a strong and re-usable foundation for distributed data ingestion and export for forthcoming Rust DataFrame efforts.

Here’s an example (slightly edited for clarity) that shows those components in action. We’ll start with a transaction log in the [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet)[^parquet] data storage format, partitioned across multiple files. From it, we’re going to calculate the busiest hours on record, measured by number of transactions:

```rust
#[derive(Data)]
struct Transaction {
    id: u64,
    time: Timestamp,
    event: String,
}

let pool = ProcessPool::new(num_processes)?;

let data_source = ParquetSource::new(vec![
    PathBuf::from("transactions-partition0.parquet"),
    PathBuf::from("transactions-partition1.parquet"),
    ...
]);

let busiest_hours = data_source
    .dist_iter()
    .map(|row| {
        let row: Transaction = row.unwrap();
        // Truncate timestamp to 1 hour granularity
        Timestamp::from_hours(row.time.as_hours())
    })
    .most_frequent(&pool, 100);

for (hour, count) in busiest_hours {
    println!("{}: approx {} transactions", hour, count);
}
```

This example spawns `num_processes` processes across a cluster. Work is distributed among them to:

 * Open and read the Parquet partitions
 * Deserialise each row as an `Transaction` and pass them to the `.map(...)` closure
 * Truncate the timestamp to 1 hour granularity
 * Probabilistically calculate the 100 most frequently occurring timestamps

The busiest hours, and the estimated number of transactions that occurred in each, are then printed:

```
2019-02-27 18:00:00: approx 1363357 transactions
2018-11-23 16:00:00: approx 1052035 transactions
2019-01-05 18:00:00: approx 999258 transactions
2019-02-13 14:00:00: approx 983379 transactions
...
```


<!--

 The work to read and analyse each Parquet partition is distributed among them. 
This is what distributed data processing looks like on Amadeus: **<!--taking computation described as an- ->leveraging the [iterator pattern](https://doc.rust-lang.org/book/ch13-02-iterators.html) to declare potentially complex transformations, which are then made to run distributed**.
-->
There are a few specific things going in this example worth calling out:

**`#[derive(Data)]`**<br/>
The `Data` trait is implemented by all types readable from `DataSource`s (a trait abstracting places data can be read *from* like files, databases and APIs) and writeable to `DataSink`s (a trait abstracting places data can be written *to*). It handles the mapping between the Rust and the native data types. It is implemented on many common types like `u32`, `String`, `HashMap<K,V>` etc.

The easiest way to implement it on your own data types is by deriving it with `#[derive(Data)]`. With this, your type is now ready to be read from any `DataSource` and written to any `DataSink`!

**`ProcessPool::new(num_processes)`**<br/>
This spawns a pool of processes. The [`constellation`](https://github.com/alecmocatta/constellation) framework can be used to automatically distribute these processes across a cluster.

**`let data_source = ParquetSource::new(vec![...])`**<br/>
We’re creating a Parquet data source from the paths of one or more Parquet files or partition directories. As it is something data can be read *from*, it is an implementor of the `DataSource` trait.

**`data_source.dist_iter()`**<br/>
To read from a `DataSource` we must first turn it into an iterator. `.iter()` turns it into a normal [`std::iter::Iterator`](https://doc.rust-lang.org/std/iter/trait.Iterator.html). There are two more powerful iterator types available: `.par_iter()` turns it into a parallel iterator (leveraging threads); and `.dist_iter()` which is what’s used here to turn it into a distributed iterator (leveraging processes).

**`.map(|row| ...)`**<br/>
`.map(...)` probably looks familiar! It’s one of many iterator adapters you’ve likely used with `std::iter::Iterator`s. <!-- Like that, lazy-->Here, the mapping closure is actually run distributed within the process pool.

**`let row: Transaction = row.unwrap()`**<br/>
`Transaction` is needed here to tell the compiler the type of the Parquet rows being read. If the type wasn’t known, `Value` could have been used instead to read data as dynamically-typed. More on this later. Unwrapping is needed as `DataSource`s return a stream of Results, to enable any errors to be handled.

**`.most_frequent(&pool, 100)`**<br/>
Run the distributed iterator on the `pool` process pool, and collect the most frequently occurring 100 timestamps and their approximated counts. A streaming algorithm, specifically a variant of the [Count–min sketch](https://en.wikipedia.org/wiki/Count–min_sketch) data structure, is used to do this probabilistically with fixed memory usage.

**And that’s it! That’s all that’s required to run distributed data analysis, on Amadeus.** Well, almost. I simplified it slightly. Take a look at the [next blog post in this series](https://github.com) for a look under the hood, where we get into an all-singing, all-dancing real-world example!<!--Let’s take a look under the hood, and then we’ll get into an all-singing, all-dancing example.-->

## Further examples
Here are some further simple examples to demonstrate the current capabilities of Amadeus.

### Printing all rows in a DataSource
```rust
use amadeus::{DataSource, data_source::Postgres, types::Value};

fn print<T: DataSource>(source: T) {
    for row in source.iter() {
        let row: Value = row.unwrap();
        println!("{:?}", row);
    }
}

fn main() {
    print(PostgresSource::new(vec![(
        "postgresql://alec@localhost/mydb".parse().unwrap(),
        "mytable".parse().unwrap(),
    )]));
}
```

### Importing Parquet data into PostgreSQL
```rust
let source = PostgresSource::new(vec![(
    "postgresql://alec@localhost/mydb".parse().unwrap(),
    "mytable".parse().unwrap(),
)]);
TODO
```

### Converting JSON to Parquet
```rust
let source = PostgresSource::new(vec![(
    "postgresql://alec@localhost/mydb".parse().unwrap(),
    "mytable".parse().unwrap(),
)]);
TODO
```

### Create Parquet
```rust
(0..1_000_000_000).into_dist_iter()
TODO
```

### Calculate π
```rust
let points = 1_000_000_000;
let within_circle = (0..points).into_dist_iter()
    .filter(|_| {
        let (x, y) = rand::random::<(f64, f64)>();
        x * x + y * y <= 1.0
    })
    .count();
let pi = 4.0 * within_circle as f64 / points as f64;
```

## Project status & Upcoming work
The focus right now is on five key areas:

 * Designing and implementing the streaming algorithms interface. **Can we create a more declarative SQL-like interface with operations like `.group_by()` that use their (probabilistic) streaming brethren under the hood?**
 * Getting process distribution working on Windows (it’s currently Linux/Mac/FreeBSD-only).
 * Exploring distributed fabric integrations like Kubernetes to increase ease of adoption.
 * Responsibilities: who can be empowered to do what, and what structure can we put in place to maximise the impact of the project.
 * And lastly, high-quality integrations with more data formats:

| Data format | `DataSource` | `DataSink` |
|---|---|---|
| CSV | ✔ | ✔ |
| JSON | ✔ | ✔ |
| XML | [👐](https://github.com) |  |
| Parquet | ✔ | [👐](https://github.com) |
| Avro | [🔨](https://github.com) |  |
| PostgreSQL | ✔ | [🔨](https://github.com) |
| HDF5 | [👐](https://github.com) |  |
| Redshift | [👐](https://github.com) |  |
| [CloudFront Logs](https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html) | ✔ | – |
[Common Crawl](http://commoncrawl.org/the-data/get-started/) | ✔ | – |
| * on S3 | [🔨](https://github.com) | [🔨](https://github.com) |
| * on HDFS | [👐](https://github.com) | [👐](https://github.com) |

✔ = Working<br/>
🔨 = Work in Progress<br/>
👐 = Requested: check out the issue for how to help!

## Call for participation
I’m keen to explore expanding this to see where it can go, so I’d love to discuss the approach, architecture and governance, as well as user stories. Feel free to strike up conversation on our discord [here](https://discord.gg)! Tell us what would make it useful for you. It’s a super important and exciting time for this project — let us know how you’d like to be a part of it!

[/r/rust thread](https://reddit.com)<br/>
[Rust Users forum thread](https://users.rust-lang.org/)<br/>

[Discord](https://discord.gg)<br/>
[GitHub](https://github.com/alecmocatta/amadeus)<br/>
[Crates.io](https://crates.io/crates/amadeus)<br/>
[Docs.rs](https://docs.rs/amadeus)





# Amadeus: Under the hood
TODO
### Process pool
Spawning a thread has a throughput and latency cost attached to it. Moreover, creating a new thread and allocating work to it is only a net gain if there is idle CPU time for the new thread to take advantage of. A good solution here is the [thread pool](https://en.wikipedia.org/wiki/Thread_pool): spawn `n` threads in one go, and allocate work among them dynamically. This is what powers [Rayon](https://github.com/rayon-rs/rayon) to achieve [“potential parallelism”](http://smallcultfollowing.com/babysteps/blog/2015/12/18/rayon-data-parallelism-in-rust/).

Threads communicate via shared memory. That is what, for example, the standard library’s [channels](https://doc.rust-lang.org/std/sync/mpsc/fn.channel.html) are built on. Processes however, particularly if they are on different machines, don’t share memory[^rdma]. Instead they communicate via message passing over sockets. This necessitates serialisation and deserialisation, and this is what makes process pools a little trickier than thread pools.

The approach taken by serde_traitobject is to make One Weird Assumption:
Trait object vtables are laid out in memory the same for every invocation of the same binary.


  A process pool is a little trickier to create than a thread pool. The reason being, TODO

### Abstraction over different data sources and sinks
What if you don’t know the type of the data and want to read it dynamically-typed? This is where **gradual typing** comes in. `Value`. TODO
### Streaming analytics functions
HyperLogLog CountMinSketch https://github.com/alecmocatta/streaming_algorithms/

Multiple in a single pass. `.group_by()` TODO

## Non-goals:

 * Non-fixed rows in memory at a time
 * DataFrame – this becomes Very Hard in a distributed scenario as memory bounding 
TODO

<!--
```rust
#[derive(Data)]
struct Event {
    id: u64,
    time: Timestamp,
    event: String,
}

let pool = ProcessPool::new(num_processes, Resources::default())?;

let source = Parquet::new(vec![
	PathBuf::from("transactions-partition0.parquet"),
	PathBuf::from("transactions-partition1.parquet"),
	...
]);

let busiest_hours = source.dist_iter()
	.map(FnMut!(|row: Result<Event, _>|
		Timestamp::from_hours(row.unwrap()
			.time
			.as_hours()
		)
	))
	.most_frequent(100, 0.99, 2.0/1000.0);

for (hour, events) in busiest_hours {
	println!("{} events during {}", events, hour);
}
```
-->

[^shared-memory]: Threads on a shared-memory supercomputer can be distributed but that’s a specialised and uncommon setup.

[^streaming-memory-consumption]: number of threads × fixed memory usage of streaming algorithm data structures.

[^parquet]: Parquet is a “column-oriented data storage format of the Apache Hadoop ecosystem” that is commonly used for this kind of data.

[^rdma]: Except RDMA, but that’s atypical.
